{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73b6e95b-0a1e-43a3-b815-47d642e95962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pima_df = pd.read_csv('C:/Users/JG/abangers/pima-indians-diabetes.csv', \n",
    "                names = ['pregnant', 'plasma', 'presure', 'thickness', 'insulin', 'BMI', 'pedigree', 'age', 'class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddbaa703-3b75-4f70-bdd9-98ef903bce5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>plasma</th>\n",
       "      <th>presure</th>\n",
       "      <th>thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pregnant  plasma  presure  thickness  insulin   BMI  pedigree  age  class\n",
       "0           6     148       72         35        0  33.6     0.627   50      1\n",
       "1           1      85       66         29        0  26.6     0.351   31      0\n",
       "2           8     183       64          0        0  23.3     0.672   32      1\n",
       "3           1      89       66         23       94  28.1     0.167   21      0\n",
       "4           0     137       40         35      168  43.1     2.288   33      1\n",
       "..        ...     ...      ...        ...      ...   ...       ...  ...    ...\n",
       "763        10     101       76         48      180  32.9     0.171   63      0\n",
       "764         2     122       70         27        0  36.8     0.340   27      0\n",
       "765         5     121       72         23      112  26.2     0.245   30      0\n",
       "766         1     126       60          0        0  30.1     0.349   47      1\n",
       "767         1      93       70         31        0  30.4     0.315   23      0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c46c2d1a-37e3-4f28-8656-677e09228cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.342342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.214815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.184466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.338235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pregnant     class\n",
       "0          0  0.342342\n",
       "1          1  0.214815\n",
       "2          2  0.184466\n",
       "3          3  0.360000\n",
       "4          4  0.338235\n",
       "5          5  0.368421\n",
       "6          6  0.320000\n",
       "7          7  0.555556\n",
       "8          8  0.578947\n",
       "9          9  0.642857\n",
       "10        10  0.416667\n",
       "11        11  0.636364\n",
       "12        12  0.444444\n",
       "13        13  0.500000\n",
       "14        14  1.000000\n",
       "15        15  1.000000\n",
       "16        17  1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['pregnant','class']].groupby(['pregnant'], as_index=False).mean().sort_values(by='pregnant', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b091c36-075a-4b77-b393-bfcd807381a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>plasma</th>\n",
       "      <th>presure</th>\n",
       "      <th>thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pregnant  plasma  presure  thickness  insulin   BMI  pedigree  age\n",
       "0           6     148       72         35        0  33.6     0.627   50\n",
       "1           1      85       66         29        0  26.6     0.351   31\n",
       "2           8     183       64          0        0  23.3     0.672   32\n",
       "3           1      89       66         23       94  28.1     0.167   21\n",
       "4           0     137       40         35      168  43.1     2.288   33\n",
       "..        ...     ...      ...        ...      ...   ...       ...  ...\n",
       "763        10     101       76         48      180  32.9     0.171   63\n",
       "764         2     122       70         27        0  36.8     0.340   27\n",
       "765         5     121       72         23      112  26.2     0.245   30\n",
       "766         1     126       60          0        0  30.1     0.349   47\n",
       "767         1      93       70         31        0  30.4     0.315   23\n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = pima_df.iloc[:,[0,1,2,3,4,5,6,7]]\n",
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e9a496f-6a30-431b-9555-4bf15657d276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "763    0\n",
       "764    0\n",
       "765    0\n",
       "766    1\n",
       "767    0\n",
       "Name: class, Length: 768, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target = pima_df['class']\n",
    "y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09f82c12-7231-401f-914c-c23d9f92f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature들의 최소 값\n",
      "pregnant     0.0\n",
      "plasma       0.0\n",
      "presure      0.0\n",
      "thickness    0.0\n",
      "insulin      0.0\n",
      "BMI          0.0\n",
      "pedigree     0.0\n",
      "age          0.0\n",
      "class        0.0\n",
      "dtype: float64\n",
      "\n",
      "feature들의 최대 값\n",
      "pregnant     1.0\n",
      "plasma       1.0\n",
      "presure      1.0\n",
      "thickness    1.0\n",
      "insulin      1.0\n",
      "BMI          1.0\n",
      "pedigree     1.0\n",
      "age          1.0\n",
      "class        1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#정규화(Normalization) : Minmax scaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "col_names = ['pregnant', 'plasma', 'presure', 'thickness', 'insulin', 'BMI', 'pedigree', 'age', 'class']\n",
    "\n",
    "# MinMaxScaler객체 생성\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# MinMaxScaler로 데이터 셋 변환.fit()과 transform()호출\n",
    "scaler.fit(pima_df)\n",
    "pima_scaled = scaler.transform(pima_df)\n",
    "\n",
    "# transfrom()할 경우 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "pima_df_scaled = pd.DataFrame(data=pima_scaled, columns=col_names)#이걸 가져다 쓰면 됨\n",
    "\n",
    "print('feature들의 최소 값')\n",
    "print(pima_df_scaled.min())\n",
    "print(\"\\nfeature들의 최대 값\")\n",
    "print(pima_df_scaled.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa0329d-9482-4fa5-99f9-8d90c115d30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>plasma</th>\n",
       "      <th>presure</th>\n",
       "      <th>thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500745</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396423</td>\n",
       "      <td>0.116567</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.919598</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347243</td>\n",
       "      <td>0.253629</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.447236</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.418778</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688442</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.198582</td>\n",
       "      <td>0.642325</td>\n",
       "      <td>0.943638</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.507538</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.490313</td>\n",
       "      <td>0.039710</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.613065</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548435</td>\n",
       "      <td>0.111870</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.608040</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.132388</td>\n",
       "      <td>0.390462</td>\n",
       "      <td>0.071307</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.633166</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448584</td>\n",
       "      <td>0.115713</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.467337</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.313131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453055</td>\n",
       "      <td>0.101196</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pregnant    plasma   presure  thickness   insulin       BMI  pedigree  \\\n",
       "0    0.352941  0.743719  0.590164   0.353535  0.000000  0.500745  0.234415   \n",
       "1    0.058824  0.427136  0.540984   0.292929  0.000000  0.396423  0.116567   \n",
       "2    0.470588  0.919598  0.524590   0.000000  0.000000  0.347243  0.253629   \n",
       "3    0.058824  0.447236  0.540984   0.232323  0.111111  0.418778  0.038002   \n",
       "4    0.000000  0.688442  0.327869   0.353535  0.198582  0.642325  0.943638   \n",
       "..        ...       ...       ...        ...       ...       ...       ...   \n",
       "763  0.588235  0.507538  0.622951   0.484848  0.212766  0.490313  0.039710   \n",
       "764  0.117647  0.613065  0.573770   0.272727  0.000000  0.548435  0.111870   \n",
       "765  0.294118  0.608040  0.590164   0.232323  0.132388  0.390462  0.071307   \n",
       "766  0.058824  0.633166  0.491803   0.000000  0.000000  0.448584  0.115713   \n",
       "767  0.058824  0.467337  0.573770   0.313131  0.000000  0.453055  0.101196   \n",
       "\n",
       "          age  class  \n",
       "0    0.483333    1.0  \n",
       "1    0.166667    0.0  \n",
       "2    0.183333    1.0  \n",
       "3    0.000000    0.0  \n",
       "4    0.200000    1.0  \n",
       "..        ...    ...  \n",
       "763  0.700000    0.0  \n",
       "764  0.100000    0.0  \n",
       "765  0.150000    0.0  \n",
       "766  0.433333    1.0  \n",
       "767  0.033333    0.0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pima_df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23aec813-c88a-4aa0-8b4f-85537743b492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV 최적 파라미터 : {'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "GridSearchCV 최고 정확도:  0.7351\n",
      "정확도 : 0.8312\n",
      "정밀도 : 0.9048\n",
      "재현율 : 0.6333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pima_df = pd.read_csv('C:/Users/JG/abangers/pima-indians-diabetes.csv', \n",
    "                names = ['pregnant', 'plasma', 'presure', 'thickness', 'insulin', 'BMI', 'pedigree', 'age', 'class'])\n",
    "\n",
    "\n",
    "#정규화(Normalization) : Minmax scaler\n",
    "col_names = ['pregnant', 'plasma', 'presure', 'thickness', 'insulin', 'BMI', 'pedigree', 'age', 'class']\n",
    "\n",
    "# MinMaxScaler객체 생성\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# MinMaxScaler로 데이터 셋 변환.fit()과 transform()호출\n",
    "scaler.fit(pima_df)\n",
    "pima_scaled = scaler.transform(pima_df)\n",
    "\n",
    "# transfrom()할 경우 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "pima_df_scaled = pd.DataFrame(data=pima_scaled, columns=col_names)#이걸 가져다 쓰면 됨\n",
    "\n",
    "X_data = pima_df_scaled.iloc[:,[0,1,2,3,4,5,6,7]]\n",
    "y_target = pima_df_scaled['class']\n",
    "\n",
    "\n",
    "#GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.1, random_state=121)\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "parameters = {'max_depth' : [1, 2, 3], 'min_samples_split' : [2, 3, 4], 'min_samples_leaf' : [2, 3, 4]}\n",
    "\n",
    "\n",
    "## refit=True가 default임. True이면 가장 좋은 파라미터 설정으로 재 학습 시킴\n",
    "pima_grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True) ##CV:cross validation 교차검증을 의미 cv=3 교차검증을 3번하겠다는 것\n",
    "pima_grid_dtree.fit(X_train, y_train)\n",
    "\n",
    "print('GridSearchCV 최적 파라미터 :', pima_grid_dtree.best_params_)\n",
    "print(\"GridSearchCV 최고 정확도: {0: .4f}\".format(pima_grid_dtree.best_score_))\n",
    "\n",
    "best_pima_grid = pima_grid_dtree.best_estimator_ #GridSearchCV를 하여 찾아낸 최적의 하이퍼 파라미터 값\n",
    "\n",
    "#GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가수행.\n",
    "dpredictions = best_pima_grid.predict(X_test)\n",
    "precision = precision_score(y_test, dpredictions)\n",
    "recall = recall_score(y_test, dpredictions)\n",
    "\n",
    "accuracy = accuracy_score(y_test, dpredictions)\n",
    "print('정확도 : {0:.4f}'.format(accuracy)) #데이터가 달라졌으니까 정확도가 올라감\n",
    "print('정밀도 : {0:.4f}'.format(precision))\n",
    "print('재현율 : {0:.4f}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7167fcc8-ba30-4028-a61c-08181693093a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "70/70 [==============================] - 1s 2ms/step - loss: 0.6438 - accuracy: 0.6541\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6129 - accuracy: 0.6541\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5927 - accuracy: 0.6657\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5720 - accuracy: 0.7192\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.7062\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5415 - accuracy: 0.7221\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5236 - accuracy: 0.7366\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.7410\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7410\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5002 - accuracy: 0.7641\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4971 - accuracy: 0.7598\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.7496\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.7685\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4771 - accuracy: 0.7742\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7627\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4749 - accuracy: 0.7757\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4691 - accuracy: 0.7771\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4743 - accuracy: 0.7815\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4655 - accuracy: 0.7728\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.7713\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4729 - accuracy: 0.7656\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.7829\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4539 - accuracy: 0.7786\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4684 - accuracy: 0.7742\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4592 - accuracy: 0.7844\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4625 - accuracy: 0.7815\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4598 - accuracy: 0.7742\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.7771\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4562 - accuracy: 0.7829\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4507 - accuracy: 0.7786\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4549 - accuracy: 0.7829\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4497 - accuracy: 0.7800\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4514 - accuracy: 0.7829\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4522 - accuracy: 0.7844\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4465 - accuracy: 0.7829\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4483 - accuracy: 0.7887\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4655 - accuracy: 0.7699\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4496 - accuracy: 0.7887\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4545 - accuracy: 0.7771\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.7786\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4403 - accuracy: 0.8090\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4419 - accuracy: 0.7959\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4526 - accuracy: 0.7699\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.7858\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4477 - accuracy: 0.7800\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4477 - accuracy: 0.7959\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.7844\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.7873\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4439 - accuracy: 0.7858\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 0.7931\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.7887\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.7916\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4473 - accuracy: 0.7713\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.7902\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.7902\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8017\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.7916\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4257 - accuracy: 0.7916\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4297 - accuracy: 0.7902\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.7988\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.7931\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4250 - accuracy: 0.7988\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8032\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4241 - accuracy: 0.7916\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.7902\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4304 - accuracy: 0.78 - 0s 3ms/step - loss: 0.4277 - accuracy: 0.7858\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4221 - accuracy: 0.8104\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.7974\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.7887\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4236 - accuracy: 0.7902\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4228 - accuracy: 0.8075\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4357 - accuracy: 0.7959\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4199 - accuracy: 0.7974\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4266 - accuracy: 0.7931\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4216 - accuracy: 0.8003\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.7988\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4193 - accuracy: 0.7974\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4159 - accuracy: 0.8003\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4165 - accuracy: 0.8046\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4168 - accuracy: 0.8017\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.4172 - accuracy: 0.8090\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4169 - accuracy: 0.8162\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4169 - accuracy: 0.8046\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4223 - accuracy: 0.7959\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4118 - accuracy: 0.8017\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4192 - accuracy: 0.7974\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4080 - accuracy: 0.8090\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4132 - accuracy: 0.8104\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4238 - accuracy: 0.7959\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4163 - accuracy: 0.8104\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4089 - accuracy: 0.8133\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4116 - accuracy: 0.8220\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4179 - accuracy: 0.7974\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.8003\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4212 - accuracy: 0.7887\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4156 - accuracy: 0.8017\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4096 - accuracy: 0.7945\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4148 - accuracy: 0.7887\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4122 - accuracy: 0.8119\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.4086 - accuracy: 0.8017\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3627 - accuracy: 0.8442\n",
      "\n",
      " Accuracy: 0.8442\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분입니다.\n",
    "np.random.seed(121)\n",
    "tf.random.set_seed(121)\n",
    "\n",
    "pima_Data_set = np.loadtxt(\"C:/Users/JG/abangers/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "#정규화(Normalization) : Minmax scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(pima_Data_set)\n",
    "pima_np_scaled = scaler.transform(pima_Data_set)\n",
    "\n",
    "X_data = pima_np_scaled[:,0:-1]\n",
    "y_target = pima_np_scaled[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.1, random_state=121)\n",
    "\n",
    "# 딥러닝 구조를 결정\n",
    "# 주어진 데이터와 input_dim으로 인한출력을 맞춰줘야함\n",
    "model = Sequential()\n",
    "model.add(Dense(40, input_dim=8, activation='PReLU'))\n",
    "# model.add(Dense(30, activation='LeakyReLU'))\n",
    "\n",
    "model.add(Dense(20, activation='ELU'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=10)\n",
    "\n",
    "print(\"\\n Accuracy: %.4f\" %(model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# model.predict() 모델 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61145d23-ec2f-459a-aa34-b2d3b301a777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 2ms/step - loss: 0.6603 - accuracy: 0.6172\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6324 - accuracy: 0.6680\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.6061 - accuracy: 0.6699\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.5798 - accuracy: 0.7031\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.7031\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5329 - accuracy: 0.7480\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7656\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.7539\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.7695\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.7734\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4684 - accuracy: 0.7695\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.7754\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4513 - accuracy: 0.7891\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4472 - accuracy: 0.8086\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4551 - accuracy: 0.7754\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4426 - accuracy: 0.7949\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.7832\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4473 - accuracy: 0.7949\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4325 - accuracy: 0.7930\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.7891\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4266 - accuracy: 0.7969\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4319 - accuracy: 0.8047\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4216 - accuracy: 0.8086\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4235 - accuracy: 0.8008\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4261 - accuracy: 0.8047\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4279 - accuracy: 0.7891\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.8066\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.7852\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.7988\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4188 - accuracy: 0.8027\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4152 - accuracy: 0.8164\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4104 - accuracy: 0.8145\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.8066\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.7969\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4158 - accuracy: 0.7988\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4175 - accuracy: 0.8027\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4060 - accuracy: 0.8008\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4116 - accuracy: 0.7969\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.8164\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4111 - accuracy: 0.8008\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.8066\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4063 - accuracy: 0.8164\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4102 - accuracy: 0.8125\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4089 - accuracy: 0.8184\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3994 - accuracy: 0.8105\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4109 - accuracy: 0.8164\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8066\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3983 - accuracy: 0.8125\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3999 - accuracy: 0.8105\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8242\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.8184\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3929 - accuracy: 0.8203\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3997 - accuracy: 0.8164\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8086\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8086\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8301\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3930 - accuracy: 0.8164\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.8145\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3814 - accuracy: 0.8223\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3789 - accuracy: 0.8281\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8164\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3847 - accuracy: 0.8223\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3762 - accuracy: 0.8262\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3914 - accuracy: 0.8203\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.8223\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3718 - accuracy: 0.8223\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3798 - accuracy: 0.8281\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8418\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3756 - accuracy: 0.8105\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3780 - accuracy: 0.8262\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8281\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3827 - accuracy: 0.8203\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3681 - accuracy: 0.8223\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8242\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3696 - accuracy: 0.8223\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8301\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.8262\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.8281\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3626 - accuracy: 0.8223\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3643 - accuracy: 0.8242\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3598 - accuracy: 0.8223\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3576 - accuracy: 0.8223\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8105\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8164\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8340\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3565 - accuracy: 0.8242\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3495 - accuracy: 0.8281\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3612 - accuracy: 0.8281\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8145\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8301\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3459 - accuracy: 0.8320\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8398\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3594 - accuracy: 0.8496\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3453 - accuracy: 0.8457\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3528 - accuracy: 0.8359\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3543 - accuracy: 0.8379\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3523 - accuracy: 0.8359\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3501 - accuracy: 0.8516\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3409 - accuracy: 0.8320\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8379\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6111 - accuracy: 0.7109\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4557 - accuracy: 0.7969\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4348 - accuracy: 0.7969\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.8086\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.7969\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.8086\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4216 - accuracy: 0.7930\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4261 - accuracy: 0.7930\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.8086\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4206 - accuracy: 0.8027\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4172 - accuracy: 0.8086\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4201 - accuracy: 0.8105\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4135 - accuracy: 0.8047\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4171 - accuracy: 0.8008\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4054 - accuracy: 0.8203\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.8008\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4023 - accuracy: 0.7988\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4027 - accuracy: 0.8086\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4103 - accuracy: 0.8066\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.8242\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4103 - accuracy: 0.7949\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4039 - accuracy: 0.8105\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.8145\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8145\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4029 - accuracy: 0.8203\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8223\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3930 - accuracy: 0.8164\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.8164\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.8164\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8223\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8047\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8105\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8262\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3847 - accuracy: 0.8164\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8066\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3875 - accuracy: 0.8125\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.8281\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3783 - accuracy: 0.8105\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3743 - accuracy: 0.8379\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3829 - accuracy: 0.8379\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8066\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3798 - accuracy: 0.8320\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8242\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3765 - accuracy: 0.8262\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3849 - accuracy: 0.8164\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3787 - accuracy: 0.8262\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3644 - accuracy: 0.8340\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3751 - accuracy: 0.8184\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8320\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3645 - accuracy: 0.8340\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3669 - accuracy: 0.8262\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3567 - accuracy: 0.8242\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3647 - accuracy: 0.8340\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.8184\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8242\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3631 - accuracy: 0.8301\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3600 - accuracy: 0.8379\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.8281\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3587 - accuracy: 0.8320\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3604 - accuracy: 0.8320\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.8281\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3586 - accuracy: 0.8281\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8340\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3482 - accuracy: 0.8301\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.8418\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3479 - accuracy: 0.8379\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3466 - accuracy: 0.8398\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3456 - accuracy: 0.8281\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.8359\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3518 - accuracy: 0.8359\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3422 - accuracy: 0.8516\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3518 - accuracy: 0.8242\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3414 - accuracy: 0.8359\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3345 - accuracy: 0.8555\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3603 - accuracy: 0.8359\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8496\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8340\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8496\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8516\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.8281\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.8438\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3345 - accuracy: 0.8418\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.8516\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3362 - accuracy: 0.8457\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3326 - accuracy: 0.8457\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8496\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3254 - accuracy: 0.8457\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8594\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3233 - accuracy: 0.8477\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3190 - accuracy: 0.8633\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8477\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.8574\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.8457\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8340\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8398\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3184 - accuracy: 0.8516\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3147 - accuracy: 0.8438\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3240 - accuracy: 0.8438\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8340\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.8477\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3087 - accuracy: 0.8633\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5187 - accuracy: 0.7539\n",
      "Epoch 1/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.7852\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4179 - accuracy: 0.8105\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4213 - accuracy: 0.8086\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4146 - accuracy: 0.8047\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8203\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8184\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3842 - accuracy: 0.8242\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3838 - accuracy: 0.8262\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3757 - accuracy: 0.8203\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3724 - accuracy: 0.8301\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3764 - accuracy: 0.8203\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8184\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3762 - accuracy: 0.8262\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8184\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3706 - accuracy: 0.8242\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8359\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.8242\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3588 - accuracy: 0.8223\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3549 - accuracy: 0.8359\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3773 - accuracy: 0.8164\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.8125\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3480 - accuracy: 0.8340\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.8359\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8535\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3529 - accuracy: 0.8262\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3514 - accuracy: 0.8105\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3480 - accuracy: 0.8301\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3410 - accuracy: 0.8359\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.8281\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.8281\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8359\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8516\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3430 - accuracy: 0.8320\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3535 - accuracy: 0.8145\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3460 - accuracy: 0.8301\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8398\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8301\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8496\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3328 - accuracy: 0.8496\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3280 - accuracy: 0.8613\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3247 - accuracy: 0.8359\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3407 - accuracy: 0.8184\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8242\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3261 - accuracy: 0.8340\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8496\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3268 - accuracy: 0.8496\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3263 - accuracy: 0.8496\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.8555\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3127 - accuracy: 0.8574\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3153 - accuracy: 0.8535\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3174 - accuracy: 0.8516\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.8418\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3115 - accuracy: 0.8496\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3120 - accuracy: 0.8516\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.8496\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8398\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.8516\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3010 - accuracy: 0.8730\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3026 - accuracy: 0.8535\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3134 - accuracy: 0.8457\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3142 - accuracy: 0.8457\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3068 - accuracy: 0.8535\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8398\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8633\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3086 - accuracy: 0.8438\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2996 - accuracy: 0.8633\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3024 - accuracy: 0.8672\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.8594\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3055 - accuracy: 0.8555\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2974 - accuracy: 0.8418\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2972 - accuracy: 0.8594\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.8691\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8633\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2966 - accuracy: 0.8574\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3072 - accuracy: 0.8477\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2936 - accuracy: 0.8809\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2854 - accuracy: 0.8535\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2857 - accuracy: 0.8672\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2931 - accuracy: 0.8613\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.8613\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2891 - accuracy: 0.8730\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2917 - accuracy: 0.8574\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2805 - accuracy: 0.8652\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2873 - accuracy: 0.8613\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2905 - accuracy: 0.8613\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.8652\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.8555\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2872 - accuracy: 0.8613\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8691\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2770 - accuracy: 0.8672\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.8672\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2780 - accuracy: 0.8730\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2805 - accuracy: 0.8730\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2841 - accuracy: 0.8652\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2764 - accuracy: 0.8555\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2726 - accuracy: 0.8672\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2902 - accuracy: 0.8594\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2739 - accuracy: 0.8789\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2721 - accuracy: 0.8652\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.8711\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.8008\n",
      "\n",
      "## 교차검증별 검증 정확도 :  [0.7109 0.7539 0.8008]\n",
      "## 평균 검증 정확도: 0.7552083333333334\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분입니다.\n",
    "np.random.seed(121)\n",
    "tf.random.set_seed(121)\n",
    "\n",
    "pima_Data_set = np.loadtxt(\"C:/Users/JG/abangers/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "#정규화(Normalization) : Minmax scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(pima_Data_set)\n",
    "pima_np_scaled = scaler.transform(pima_Data_set)\n",
    "\n",
    "X_data = pima_np_scaled[:,0:-1]\n",
    "Y_target = pima_np_scaled[:,-1]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.1, random_state=121)\n",
    "\n",
    "# 딥러닝 구조를 결정\n",
    "#주어진 데이터와 input_dim으로 인한출력을 맞춰줘야함\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=8, activation='PReLU'))\n",
    "# model.add(Dense(15, activation='LeakyReLU'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='ELU'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=3)\n",
    "cv_accuracy = []\n",
    "for train_index, test_index in skfold.split(X_data, y_target): \n",
    "\n",
    "    X_train, X_test = X_data[train_index], X_data[test_index] \n",
    "    y_train, y_test = y_target[train_index], y_target[test_index] \n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=10)\n",
    "#     print(\"\\n Accuracy: %.4f\" %(model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "\n",
    "    cv_accuracy.append(model.evaluate(X_test, y_test)[1])\n",
    "    \n",
    "#교차 검증별 정확도 및 평균 정확도 계산\n",
    "print('\\n## 교차검증별 검증 정확도 : ', np.round(cv_accuracy, 4))\n",
    "print(\"## 평균 검증 정확도:\", np.mean(cv_accuracy)) ##이산적인 데이터에만 사용, 회귀같이 연속데이터에는 사용 불가\n",
    "\n",
    "\n",
    "# model.predict() 모델 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b8aa5-1b20-4463-bab1-03351f560160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
